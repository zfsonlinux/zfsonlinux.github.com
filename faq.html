<html>
<head>
<title>ZFS on Linux</title>
<meta name="keyword" content="zfs, linux" />
<meta name="description" content="ZFS on Linux FAQ." />
<meta name="robots" content="all" />
</head>
<body>
<center>
<img src="images/zfs-linux.png">

<table width=80%>
	<tr bgcolor="#aaaaaa">
		<th> FAQ - Table of Contents</th>
	</tr>
	<tr>
		<td>
<ul>
	<li><a href="#WhatAboutTheLicensingIssue">
	1.1 What about the licensing issue?</a></li>
	<li><a href="#HowDoIInstallIt">
	1.2 How do I install it?</a></li>
	<li><a href="#WhyDoesntItBuild">
	1.3 Why doesn&#8217;t it build?</a></li>
	<li><a href="#HowDoIMountTheFileSystem">
	1.4 How do I mount the file system?</a></li>
	<li><a href="#WhyShouldIUseA64BitSystem">
	1.5 Why should I use a 64-bit system?</a></li>
	<li><a href="#WhatKernelVersionsAreSupported">
	1.6 What kernel versions are supported?</a></li>
	<li><a href="#WhatDevNamesShouldIUseWhenCreatingMyPool">
	1.7 What /dev/ names should I used when creating my pool?</a></li>
	<li><a href="#HowDoIChangeNamesOnAnExistingPool">
	1.8 How do I change the /dev/ names on an existing pool?</a></li>
	<li><a href="#HowDoISetupZdevConf">
	1.9 How do I setup the /etc/zfs/zdev.conf file?</a></li>
	<li><a href="#PerformanceConsideration">
	1.10 What&#8217;s going on with performance?</a></li>
	<li><a href="#WhatDoesZpoolCacheDo">
	1.11 What does the /etc/zfs/zpool.cache file do?</a></li>
	<li><a href="#HowDoISetupShares">
	1.12 How do I setup an NFS or SMB shares?</a></li>
	<li><a href="#CanIBootFromZFS">
	1.13 Can I boot from ZFS root?</a></li>
	<li><a href="#HowDoIAutomaticallyMountZFSFilesystemsDuringStartup">
	1.14 How do I automatically mount ZFS filesystems during startup?</a></li>
	<li><a href="#HowDoesZFSonLinuxHandlesAdvacedFormatDrives">
	1.15 How does ZFS on Linux handles Advanced Format (4 KB sector) drives?</a></li>
	<li><a href="#DoIHaveToUseECCMemory">
	1.16 Do I have to use ECC memory?</a></li>
	<li><a href="#CanIUseaZVOLforSwap">
	1.17 Can I use a ZVOL for swap?</a></li>
	<li><a href="#HowCanIHelp">
	2.1 How can I help?</a></li>
</ul>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="WhatAboutTheLicensingIssue">
1.1 What about the licensing issue?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>In a nutshell, the issue is that the Linux kernel which is licensed under the 
<a href="http://www.gnu.org/licenses/gpl.html">GNU General Public License</a> is
<a href="http://www.gnu.org/licenses/license-list.html#CDDL">incompatible</a>
with ZFS which is licensed under the Sun <a
href="http://hub.opensolaris.org/bin/view/Main/opensolaris_license">CDDL</a>.
While both the GPL and CDDL are open source licenses their terms are such that
it is impossible to simultaneously satisfy both licenses.  This means that a
single <em>derived work</em> of the Linux kernel and ZFS cannot be legally
<em>distributed</em>.</p>

<p>One way to resolve this issue is to implement
<a href="http://zfs-fuse.net/">ZFS in user space with FUSE</a> where it is not
considered a <em>derived work</em> of the kernel.  This approach resolves the
licensing issues but it has some technical drawbacks.  There is another option
though.  The CDDL does not restrict modification and release of the ZFS source
code which is <a href="http://dlc.sun.com/osol/on/downloads/current/">publicly
available</a> as part of OpenSolaris.  The ZFS code can be modified to build
as a CDDL licensed kernel module which is <em>not distributed</em> as part of
the Linux kernel.  This makes a Native ZFS on Linux implementation possible if
you are willing to
<a href="http://github.com/zfsonlinux/zfs/downloads">download</a> and
<a href="zfs-building-rpm.html">build it</a> yourself.</p>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="HowDoIInstallIt">
1.2 How do I install it?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>If you are using an Ubuntu distribution the easiest way to install ZFS is
with the <a href="https://launchpad.net/~zfs-native/+archive/stable">
ZFS PPA</a>.  Simply add the PPA to your list of repositories and install
the ubuntu-zfs packages with your favorite package manager.  The ZFS source will
be downloaded to your system and compiled with dkms.  In addition, the ZFS
source will be automatically recompiled when a new version is available
or when you update your kernel.</p>

<pre>
$ sudo add-apt-repository ppa:zfs-native/stable
$ sudo apt-get update
$ sudo apt-get install ubuntu-zfs
</pre>

<p>If you are using a different Linux distribution you will have to manually
<a href="spl-building-rpm.html">download and compile the source</a> as
described in the documentation.</p>

<p>Note that because we want to take a cautious, cafeful approach with
ZFS on Linux we are still calling this a release candiate.  However, all
of the core functionality is in place and most of the advanced features
are working.  Stability of the latest release candidates has been very good
and performance is respectible.  Many people are successfully using the
ZFS on Linux release candidates every day.</p>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="WhyDoesntItBuild">
1.3 Why doesn&#8217;t it build?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>Building a kernel module against an arbitrary kernel version is a
complicated thing to do.  Every Linux distribution has their own idea of
how this should be done.  It depends on the base kernel version, any
distribution specific patches, and exactly how the kernel was configured.
If you run in to problems here are few thing to check.  If none of these
things explain your problem, then please open a new
<a href="http://github.com/zfsonlinux/zfs/issues">issue</a>
which fully describes the problem.</p>

<ul>
	<li>The kernel API changes frequently, currently 2.6.26 through 3.3 kernels are supported.
	</li>
	<li>Check for known <a
href="http://github.com/zfsonlinux/spl/issues/labels/Build%20Issue">SPL</a> and
<a href="http://github.com/zfsonlinux/zfs/issues/labels/Build%20Issue">ZFS</a>
build issues.</li>
	<li>There is also a list of
<a href="#WhatKernelVersionsAreSupported">
tested platforms</a> you might find helpful.</li>
	<li>Make sure CONFIG_PREEMPT is disabled in your kernel
(<a href="http://github.com/zfsonlinux/zfs/issues/#issue/83">issue 83</a>).</li>
</ul>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="HowDoIMountTheFileSystem">
1.4 How do I mount the file system?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>A mountable dataset will be created and automatically mounted when you
first create the pool with <i>zpool create</i>.  Additional datasets can be
created with <i>zfs create</i> and they will be automatically mounted.</p>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="WhyShouldIUseA64BitSystem">
1.5 Why should I use a 64-bit system?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>In the Solaris kernel it is common practice to make heavy use of the
virtual address space because it is designed to work well. However, in
the Linux kernel most memory is addressed with a physical address and use of
the virtual address space is strongly discouraged. This is particularly true
on 32-bit arches where the virtual address space is limited to roughly 100MiB
by default. Using the virtual address space on 64-bit Linux kernels is also
discouraged.  But in this case the address space is so much larger than
physical memory it is not as much of an issue.</p>

<p>If you are bumping up against the virtual memory limit you will see
this message in your system logs.  You can increase the virtual address size
with the boot option <em>vmalloc=512M</em>.</p>

<pre>
vmap allocation for size 4198400 failed: use vmalloc=&lt;size&gt; to increase
size.
</pre>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="WhatKernelVersionsAreSupported">
1.6  What kernel versions are supported?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>The latest spl/zfs-.0.6.0-rc8 release candidate supports Linux <b>2.6.26 -
3.3.0</b> kernels.  This covers most of the kernels used in the major Linux
distributions.  The following distributions are all tested at LLNL using a
buildbot based continuous integration development model.  If you need support for
a newer kernel you may find it in the latest github sources. </p>

<ul>
	<li>RHEL 6.0 (Santiago) Desktop - AMD64</li>
	<li>TOSS 2.0 Desktop - AMD64</li>
	<li>Fedora 13 (Goddard) Desktop - AMD64</li>
	<li>Fedora 14 (Laughlin) Desktop - AMD64</li>
	<li>Fedora 15 (Lovelock) Desktop - AMD64</li>
	<li>Fedora 16 (Verne) Desktop - AMD64</li>
	<li>Ubuntu 10.04 (Lucid) Desktop - AMD64</li>
	<li>Ubuntu 10.10 (Maverick) Desktop - AMD64</li>
	<li>Ubuntu 11.04 (Natty) Desktop - AMD64</li>
	<li>Ubuntu 11.10 (Oneiric) Desktop - AMD64</li>
	<li>Debian 5.0 (Lenny) Desktop - AMD64</li>
	<li>Debian 6.0 (Squeeze) Desktop - AMD64</li>
	<li>openSUSE 11.4 (Celadon) Desktop - AMD6</li>
	<li>ArchLinux (Current) Desktop - AMD64</li>
</ul>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="WhatDevNamesShouldIUseWhenCreatingMyPool">
1.7 What /dev/ names should I use when creating my pool?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>There are different /dev/ names that can be used when creating a ZFS pool.
Each option has advantages and drawbacks, the right choice for your ZFS
pool really depends on your requirements.  For development and testing
using /dev/sdX naming is quick and easy.  A typical home server might
prefer /dev/disk/by-id/ naming for simplicity and readability.  While very
large configurations with multiple controllers, enclosures, and switches
will likely prefer /dev/disk/zpool naming for maximum control.  But in
the end, how you choose to identify your disks is up to you.</p>
<ul>
	<li>
<b>/dev/sdX, /dev/hdX:</b> Best for development/test pools
	</li>
	<ul>
		<li>
<em>Summary:</em>
The top level /dev/ names are the default for consistency with other ZFS
implementations.  They are available under all Linux distributions and are
commonly used.  However, because they are not persistent they should only
be used with ZFS for development/test pools.
		</li>
		<li>
<em>Benefits:</em>This method is easy for a quick test, the names are short,
and they will be available on all Linux distributions.
		</li>
		<li>
<em>Drawbacks:</em>The names are not persistent and will change depending on
what order they disks are detected in.  Adding or removing hardware for your
system can easily cause the names to change.  You would then need to remove
the zpool.cache file and re-import the pool using the new names.
		</li>
		<li>
<em>Example:</em>
<pre>$ sudo zpool create tank sda sdb</pre>
		</li>
	</ul><br>

	<li>
<b>/dev/disk/by-id/:</b> Best for small pools (less than 10 disks)
	</li>
	<ul>
		<li>
<em>Summary:</em>
This directory contains disk identifiers with more human readable names.
The disk identifier usually consists of the interface type, vendor name,
model number, device serial number, and partition number.  This approach
is more user friendly because it simplifies identifying a specific disk.
		</li>
		<li>
<em>Benefits:</em>
Nice for small systems with a single disk controller.  Because the names
are persistent and guaranteed not to change, it doesn't matter how the disks
are attached to the system.  You can take them all out, randomly mixed them
up on the desk, put them back anywhere in the system and your pool will still
be automatically imported correctly.
		</li>
		<li>
<em>Drawbacks:</em>
Configuring redundancy groups based on physical location becomes difficult
and error prone.
		</li>
		<li>
<em>Example:</em>
<pre>$ sudo zpool create tank scsi-SATA_Hitachi_HTS7220071201DP1D10DGG6HMRP</pre>
		</li>
	</ul><br>

	<li>
<b>/dev/disk/by-path/:</b> Good for large pools (greater than 10 disks)
	</li>
	<ul>
		<li>
<em>Summary:</em>
This approach is to use device names which include the physical cable layout
in the system, which means that a particular disk is tied to a specific
location.  The name describes the PCI bus number, as well as enclosure names
and port numbers.  This allows the most control when configuring a large pool.
		</li>
		<li>
<em>Benefits:</em>
Encoding the storage topology in the name is not only helpful for locating a
disk in large installations.  But it also allows you to explicitly layout your
redundancy groups over multiple adapters or enclosures.
		</li>
		<li>
<em>Drawbacks:</em>
These names are long, cumbersome, and difficult for a human to manage.
		</li>
		<li>
<em>Example:</em>
<pre>$ sudo zpool create tank pci-0000:00:1f.2-scsi-0:0:0:0 pci-0000:00:1f.2-scsi-1:0:0:0</pre>
		</li>
	</ul><br>

	<li>
<b>/dev/disk/zpool/:</b> Best for large pools (greater than 10 disks)
	</li>
	<ul>
		<li>
<em>Summary:</em>
This is essentially a variation of the /dev/disk/by-path/ method which allows
you to pick your own unique meaningful names for the disks.  These names will
be displayed by all the zfs utilities so it can be used to clarify the
administration of a large complex pool.
		</li>
		<li>
<em>Benefits:</em>
This approach has all the benefits of the /dev/disk/by-path/ method plus
it allows you to choose meaningful human readable names.
		</li>
		<li>
<em>Drawbacks:</em>
This method relies on having a /etc/zfs/zdev.conf file properly configured
for your system.   To configure this file please refer to section
<a href="#HowDoISetupZdevConf">1.9 How do I setup the /etc/zfs/zdev.conf file?</a>
		</li>
		<li>
<em>Example:</em>
<pre>$ sudo zpool create tank mirror A1 B1 mirror A2 B2</pre>
		</li>
	</ul>
</ul>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="HowDoIChangeNamesOnAnExistingPool">
1.8 How do I change the /dev/ names on an existing pool?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>Changing the /dev/ names on an existing pool can be done by simply
exporting the pool and re-importing it with the -d option to specify
which new names should be used. For example, to use the custom names
in /dev/disk/zpool:</p>

<pre>
$ sudo zpool export tank
$ sudo zpool import -d /dev/disk/zpool tank
</pre>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="HowDoISetupZdevConf">
1.9 How do I setup the /etc/zfs/zdev.conf file?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>In order to use /dev/disk/zpool/ naming the /etc/zfs/zdev.conf must be
configured.  This configuration file consists of custom meaningful names
which are mapped to a /dev/disk/by-path identifier.</p>

<p>For example, if you have a Sun X4550 and would like to setup mirror
pairs across controllers.  You can create a configuration file which
assigns the names A[0-7] to the disks on the first controller, and the
names B[0-7] to the disks on the second controller.

<pre>
$ cat /etc/zfs/zdev.conf
#
# Custom by-path mapping for large JBOD configurations
#
# Example Config:
# Sun x4550 for RHEL6
#

# Channel A: PCI Bus 2
A0      pci-0000:02:00.0-sas-0x50062b0000000001:1:0-0xd6807184d601e192:0
A1      pci-0000:02:00.0-sas-0x50062b0000000002:1:1-0xd4905378e6e3d592:1
A2      pci-0000:02:00.0-sas-0x50062b0000000003:1:2-0xd3827673d806d392:2
A3      pci-0000:02:00.0-sas-0x50062b0000000004:1:3-0xd6805385d6e3e192:3
A4      pci-0000:02:00.0-sas-0x50062b0000000005:1:4-0xd680655bd6f5b792:4
A5      pci-0000:02:00.0-sas-0x50062b0000000006:1:5-0x7a967598ec06d091:5
A6      pci-0000:02:00.0-sas-0x50062b0000000007:1:6-0xd3826c60d8fcbf92:6
A7      pci-0000:02:00.0-sas-0x50062b0000000008:1:7-0xd6805271d6e2cd92:7

# Channel B: PCI Bus 3
B0      pci-0000:03:00.0-sas-0x50062b0000000002:1:0-0xd680685fd6f8bb92:0
B1      pci-0000:03:00.0-sas-0x50062b0000000003:1:1-0xd58c706de200cb92:1
B2      pci-0000:03:00.0-sas-0x50062b0000000004:1:2-0xd5897480df04de92:2
B3      pci-0000:03:00.0-sas-0x50062b0000000005:1:3-0xd6805764d6e7c092:3
B4      pci-0000:03:00.0-sas-0x50062b0000000006:1:4-0xd6806a6dd6fac992:4
B5      pci-0000:03:00.0-sas-0x50062b0000000007:1:5-0xd58c6b84e2fbe192:5
B6      pci-0000:03:00.0-sas-0x50062b0000000008:1:6-0xd58a576ee0e7cb92:6
B7      pci-0000:03:00.0-sas-0x50062b0000000009:1:7-0xd5877871dd08cf92:7
</pre>

<p>After defining the new disk names run <em>udevadm trigger</em> to
prompt udev to parse the configuration file.  This will result in a new
/dev/disk/zpool directory which is populated with symlinks to /dev/sdX names.
You can then create the new pool of mirrors with the following command:</p>

<pre>
$ sudo zpool create tank \
	mirror A0 B0 mirror A1 B1 mirror A2 B2 mirror A3 B3 \
	mirror A4 B4 mirror A5 B5 mirror A6 B6 mirror A7 B7

$ sudo zpool status
  pool: tank
 state: ONLINE
 scan: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	tank        ONLINE       0     0     0
	  mirror-0  ONLINE       0     0     0
	    A0      ONLINE       0     0     0
	    B0      ONLINE       0     0     0
	  mirror-1  ONLINE       0     0     0
	    A1      ONLINE       0     0     0
	    B1      ONLINE       0     0     0
	  mirror-2  ONLINE       0     0     0
	    A2      ONLINE       0     0     0
	    B2      ONLINE       0     0     0
	  mirror-3  ONLINE       0     0     0
	    A3      ONLINE       0     0     0
	    B3      ONLINE       0     0     0
	  mirror-4  ONLINE       0     0     0
	    A4      ONLINE       0     0     0
	    B4      ONLINE       0     0     0
	  mirror-5  ONLINE       0     0     0
	    A5      ONLINE       0     0     0
	    B5      ONLINE       0     0     0
	  mirror-6  ONLINE       0     0     0
	    A6      ONLINE       0     0     0
	    B6      ONLINE       0     0     0
	  mirror-7  ONLINE       0     0     0
	    A7      ONLINE       0     0     0
	    B7      ONLINE       0     0     0

errors: No known data errors
</pre>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="PerformanceConsideration">
1.10 What&#8217;s going on with performance?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>To achieve good performance with your pool there are some easy best
practices you should follow.  Additionally, it should be made clear that
the ZFS on Linux implementation has not yet been optimized for performance.
As the project matures we can expect performance to improve.</p>

<ul>
	<li>
<b>Evenly balance your disk across controllers:</b>
Often the limiting factor for performance is not the disk but the controller.
By balancing your disks evenly across controllers you can often improve
throughput.
	</li>
	<li>
<b>Create your pool using whole disks:</b>
When running <em>zpool create</em> use whole disk names.  This will allow
ZFS to automatically partition the disk to ensure correct alignment.  It
will also improve interoperability with other ZFS implementations which
honor the <em>wholedisk</em> property.
	</li>
	<li>
<b>Have enough memory:</b>
A minimum of 2GB of memory is recommended for ZFS.  Additional memory is
strongly recommended when the compression and deduplication features are
enabled.
	</li>
	<li>
<b>Improve performance by setting ashift=12:</b>
You may be able to improve performance for some workloads by setting
ashift=12.  This tuning can only be set when the pool is first created
and it will result in a decrease of capacity.  For additional detail on why
you should set this option when using Advanced Format drives see section
<a href="#HowDoesZFSonLinuxHandlesAdvacedFormatDrives">
1.15 How does ZFS on Linux handles Advanced Format disks?</a>
	</li>
</ul>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="WhatDoesZpoolCacheDo">
1.11 What does the /etc/zfs/zpool.cache file do?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>Whenever a pool is imported in the system it will be added to the
/etc/zfs/zpool.cache file.  This file stores pool configuration information
such as the vdev device names and the active pool state.  If this file
exists when the ZFS modules are loaded then any pool listed in the cache
file will be automatically imported.  When a pool is not listed in the
cache file it will need to be explicitly imported.
</p>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="HowDoISetupShares">
1.12 How do I setup an NFS or SMB shares?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>ZFS has been integrated with the Linux NFS server.  You can share
a ZFS file system by setting the <em>sharenfs</em> file system property.
For example, here's how to share the file system tank/home with the
default NFS options.  Note you must still manually configure your
network to allow NFS.</p>

<pre>
$ sudo zfs set sharenfs=on tank/home
</pre>

<p>ZFS has not yet been directly integrated with the Linux Samba server.
This means that setting the <em>sharesmb</em> file system property has
no effect.  To share a ZFS file system with Samba you must manually
configure the Samba server.</p>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="CanIBootFromZFS">
1.13 Can I boot from ZFS?</a>
		</th>
	</tr>
	<tr>
		<td>
Yes, but booting from ZFS is currently not recommended without a patched
version of grub.  However, this does not mean you can't use ZFS as your
root file system.  You can configure your system with a small ext4 /boot
file system and refer to the following <a href="https://github.com/
zfsonlinux/zfs/blob/master/dracut/README.dracut.markdown">
documentation</a> for setting up a ZFS root file system with dracut.
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="HowDoIAutomaticallyMountZFSFilesystemsDuringStartup">
1.14 How do I automatically mount ZFS file systems during startup?</a>
		</th>
	</tr>
	<tr>
		<td>
<ul>
	<li>
<b>Ubuntu PPA:</b>
Auto mounting is provided by the enhanced mountall package from the 
<a href="https://launchpad.net/~zfs-native/+archive/stable">ZFS PPA</a>.
Install the ubuntu-zfs package to get this feature.
	</li>
	<li>
<b>Fedora, RHEL, Arch, Gentoo, Lunar:</b>
Init scripts for these distributions have been provided.  If your
distribution of choice isn't represented please submit an init script
modeled on one of these so we can include it.
	</li>
</ul>

<p>Note that the SELinux policy for ZFS on Linux is not yet implemented.
This can lead to issues such as the init script failing to auto-mount the
filesystems when SELinux is set to enforce.  The long term solution is to
add ZFS as a known filesystem type which supports xattrs to the default
SELinux policy.  This is something which must be done by the upstream
Linux distribution.  In the mean time, you can workaround this by setting
SELinux to permissive or disabled.</p>

<pre>
$ cat /etc/selinux/config
# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=disabled
# SELINUXTYPE= can take one of these two values:
#     targeted - Targeted processes are protected,
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted
</pre>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="HowDoesZFSonLinuxHandlesAdvacedFormatDrives">
1.15 How does ZFS on Linux handles Advanced Format disks?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>Advanced Format (AF) is a new disk format which natively uses a 4,096
byte instead of 512 byte sector size. To maintain compatibility with legacy
systems AF disks emulate a sector size of 512 bytes. By default, ZFS will
automatically detect the sector size of the drive. This combination will
result in poorly aligned disk access which will greatly degrade the pool
performance.</p>

Starting with version 0.6.0-rc5 an ashift property has been added to the
zpool command to explicitly assign the sector size of the created pool.
The ashift values range from 9 to 16 with the default value 0 meaning
auto-detect the sector size. This value is actually a bit shift value,
so an ashift value for 512 bytes is 9 (2<sup>9</sup> = 512) while the
ashift value for 4,096 bytes is 12 (2<sup>12</sup> = 4,096).  To force
the pool to use 4,096 byte sectors we must specify this at pool creation
time:</p>

<pre>$ sudo zpool create -o ashift=12 tank mirror sda sdb</pre>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="DoIHaveToUseECCMemory">
1.16 Do I have to use ECC memory for ZFS?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>Using ECC memory for ZFS is strongly recommended for enterprise
environments where the strongest data integrity guarantees are required.
Without ECC memory rare random bit flips caused by cosmic rays or by
faulty memory can go undetected.  If this were to occur ZFS (or any
other filesystem) will write the damaged data to disk and be unable to
automatically detect the corruption.</p>

<p>Unfortunately, ECC memory is not always supported by consumer grade
hardware.  And even when it is ECC memory will be more expensive.  For
home users the additional safety brought by ECC memory might not justify
the cost.  It's up to you to determine what level of protection your
data requires.</p>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="CanIUseaZVOLforSwap">
1.17 Can I use a ZVOL for swap?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>Not yet.  Currently using a ZVOL for a swap device is not stable,
see <a href=http://github.com/zfsonlinux/zfs/issues/342">issue #342</a>
for futher details.  This should be possible in the future.</p>
		</td>
	</tr>
	<tr bgcolor="#aaaaaa">
		<th>
<a name="HowCanIHelp">
2.1 How can I help?</a>
		</th>
	</tr>
	<tr>
		<td>
<p>The most helpful thing you can do is to try ZFS on your Linux
system and report any
<a href="http://github.com/zfsonlinux/zfs/issues">issues</a>.  If you
like what you see and would like to contribute to the project please send
<a href="http://github.com/behlendorf">me</a> an email.  There are quite a few
open issues on the <a href="http://github.com/zfsonlinux/zfs/issues">issue
tracker</a> which need attention or if you have an idea of your own that is
fine too.</p>
		</td>
	</tr>
</table>

</center>
</body>
</html>
